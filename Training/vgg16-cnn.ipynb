{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1841561,"sourceType":"datasetVersion","datasetId":930779}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom keras.optimizers import Adam\nfrom keras.layers import (Conv2D, Flatten, MaxPooling2D, Input, \n                          BatchNormalization, Dropout, Dense, InputLayer, GlobalAveragePooling2D)\nfrom keras.models import Model\n\ndef create_combined_model(input_shape=(48, 48, 1), num_classes=8):\n    vgg16_base = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n\n    input_layer = tf.keras.layers.Input(shape=input_shape)\n    x = tf.keras.layers.Conv2D(3, (1, 1), padding='same')(input_layer)  # Repeat grayscale channel to 3 channels\n    x = vgg16_base(x)\n\n    # Custom layers added after VGG16\n    x = Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n\n    x = Conv2D(filters=384, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)  # Removed pooling to prevent dimension reduction\n\n    x = Conv2D(filters=192, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n\n    x = Conv2D(filters=384, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n\n    # Instead of MaxPooling, we use GlobalAveragePooling2D to handle the dimensions properly\n    x = GlobalAveragePooling2D()(x)\n\n    x = Dense(256, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n\n    output_layer = Dense(num_classes, activation='softmax')(x)\n\n    return Model(inputs=input_layer, outputs=output_layer, name='combined_model')","metadata":{"execution":{"iopub.status.busy":"2024-06-15T16:36:01.126169Z","iopub.execute_input":"2024-06-15T16:36:01.127026Z","iopub.status.idle":"2024-06-15T16:36:15.001785Z","shell.execute_reply.started":"2024-06-15T16:36:01.126979Z","shell.execute_reply":"2024-06-15T16:36:15.000827Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-15 16:36:03.390939: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-15 16:36:03.391044: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-15 16:36:03.555130: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"def train():\n    log_dir = './log'\n    train_dataset_path = r\"/kaggle/input/google-fer-image-format/train\"\n    val_dataset_path = r'/kaggle/input/google-fer-image-format/val'\n    batch_size = 64\n    lr = 1e-3\n    epochs = 720\n    num_classes = 8\n\n    train_datagen = ImageDataGenerator(\n        rescale=1 / 255.0,\n        rotation_range=10,\n        zoom_range=0.1,\n        horizontal_flip=True\n    )\n\n    train_generator = train_datagen.flow_from_directory(\n        directory=train_dataset_path,\n        target_size=(48, 48),\n        color_mode=\"grayscale\",\n        batch_size=batch_size,\n        class_mode=\"categorical\",\n        shuffle=True,\n        seed=42\n    )\n\n    test_datagen = ImageDataGenerator(rescale=1 / 255.0)\n\n    valid_generator = test_datagen.flow_from_directory(\n        directory=val_dataset_path,\n        target_size=(48, 48),\n        color_mode=\"grayscale\",\n        class_mode=\"categorical\",\n        batch_size=batch_size,\n        shuffle=True,\n        seed=42\n    )\n\n    model = create_combined_model(num_classes=num_classes)\n    model.summary()\n\n    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n#     reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n    tensorboard = TensorBoard(log_dir=log_dir)\n    \n    optimizer = Adam(learning_rate=lr)\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=optimizer)\n\n    model.fit(\n        train_generator,\n        validation_data=valid_generator,\n        epochs=epochs,\n        callbacks=[tensorboard, early_stopping]\n    )\n\n    model.evaluate(valid_generator, verbose=1)\n    model.save('./vgg_custom.h5')\n\nif __name__ == '__main__':\n    train()","metadata":{"execution":{"iopub.status.busy":"2024-06-15T16:36:19.740203Z","iopub.execute_input":"2024-06-15T16:36:19.741270Z","iopub.status.idle":"2024-06-15T17:50:28.981398Z","shell.execute_reply.started":"2024-06-15T16:36:19.741231Z","shell.execute_reply":"2024-06-15T17:50:28.980384Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Found 32645 images belonging to 8 classes.\nFound 8166 images belonging to 8 classes.\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"combined_model\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"combined_model\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m6\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │    \u001b[38;5;34m14,714,688\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │     \u001b[38;5;34m1,179,904\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │         \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m384\u001b[0m)      │       \u001b[38;5;34m885,120\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m384\u001b[0m)      │         \u001b[38;5;34m1,536\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m384\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m192\u001b[0m)      │       \u001b[38;5;34m663,744\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m192\u001b[0m)      │           \u001b[38;5;34m768\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m192\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m384\u001b[0m)      │       \u001b[38;5;34m663,936\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m384\u001b[0m)      │         \u001b[38;5;34m1,536\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m384\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m98,560\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │         \u001b[38;5;34m2,056\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,179,904</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">885,120</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">663,744</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">663,936</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,560</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,056</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,213,902\u001b[0m (69.48 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,213,902</span> (69.48 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,210,958\u001b[0m (69.47 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,210,958</span> (69.47 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,944\u001b[0m (11.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,944</span> (11.50 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/720\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1718469437.067572     129 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1718469437.097886     129 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.1584 - loss: 2.4590","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718469557.464018     132 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 289ms/step - accuracy: 0.1584 - loss: 2.4587 - val_accuracy: 0.1286 - val_loss: 3.0486\nEpoch 2/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 91ms/step - accuracy: 0.1925 - loss: 2.0688 - val_accuracy: 0.2066 - val_loss: 2.0936\nEpoch 3/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 91ms/step - accuracy: 0.2164 - loss: 2.0002 - val_accuracy: 0.2627 - val_loss: 1.8023\nEpoch 4/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 90ms/step - accuracy: 0.2570 - loss: 1.8554 - val_accuracy: 0.0699 - val_loss: 31.3341\nEpoch 5/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 89ms/step - accuracy: 0.2669 - loss: 1.8126 - val_accuracy: 0.2394 - val_loss: 1.8635\nEpoch 6/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 87ms/step - accuracy: 0.2671 - loss: 1.7850 - val_accuracy: 0.2803 - val_loss: 1.7275\nEpoch 7/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 90ms/step - accuracy: 0.2629 - loss: 1.7843 - val_accuracy: 0.0643 - val_loss: 51.7985\nEpoch 8/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 91ms/step - accuracy: 0.2716 - loss: 1.7687 - val_accuracy: 0.1523 - val_loss: 2.2070\nEpoch 9/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 89ms/step - accuracy: 0.2996 - loss: 1.7432 - val_accuracy: 0.2693 - val_loss: 3.3539\nEpoch 10/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 90ms/step - accuracy: 0.3276 - loss: 1.6850 - val_accuracy: 0.2744 - val_loss: 2.2766\nEpoch 11/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 89ms/step - accuracy: 0.3462 - loss: 1.6497 - val_accuracy: 0.3829 - val_loss: 1.5013\nEpoch 12/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 88ms/step - accuracy: 0.3592 - loss: 1.5941 - val_accuracy: 0.3486 - val_loss: 1.6039\nEpoch 13/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 89ms/step - accuracy: 0.3697 - loss: 1.5779 - val_accuracy: 0.4142 - val_loss: 1.4326\nEpoch 14/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 88ms/step - accuracy: 0.3834 - loss: 1.5377 - val_accuracy: 0.3114 - val_loss: 1.6807\nEpoch 15/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 89ms/step - accuracy: 0.3848 - loss: 1.5317 - val_accuracy: 0.4088 - val_loss: 1.4623\nEpoch 16/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 103ms/step - accuracy: 0.3970 - loss: 1.5024 - val_accuracy: 0.4161 - val_loss: 1.3908\nEpoch 17/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 119ms/step - accuracy: 0.4044 - loss: 1.4752 - val_accuracy: 0.3746 - val_loss: 1.5097\nEpoch 18/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 112ms/step - accuracy: 0.4136 - loss: 1.4550 - val_accuracy: 0.3292 - val_loss: 1.6866\nEpoch 19/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 119ms/step - accuracy: 0.4152 - loss: 1.4478 - val_accuracy: 0.4351 - val_loss: 1.3981\nEpoch 20/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 122ms/step - accuracy: 0.4221 - loss: 1.4363 - val_accuracy: 0.2895 - val_loss: 1.8219\nEpoch 21/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 116ms/step - accuracy: 0.4272 - loss: 1.4348 - val_accuracy: 0.3234 - val_loss: 1.5732\nEpoch 22/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 104ms/step - accuracy: 0.4387 - loss: 1.4018 - val_accuracy: 0.4440 - val_loss: 1.3508\nEpoch 23/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 101ms/step - accuracy: 0.4342 - loss: 1.4096 - val_accuracy: 0.3862 - val_loss: 1.4531\nEpoch 24/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 93ms/step - accuracy: 0.4462 - loss: 1.3897 - val_accuracy: 0.3304 - val_loss: 1.6101\nEpoch 25/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 91ms/step - accuracy: 0.4441 - loss: 1.3920 - val_accuracy: 0.4053 - val_loss: 1.4123\nEpoch 26/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 93ms/step - accuracy: 0.4487 - loss: 1.3752 - val_accuracy: 0.4313 - val_loss: 1.3868\nEpoch 27/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 92ms/step - accuracy: 0.4576 - loss: 1.3737 - val_accuracy: 0.4767 - val_loss: 1.3258\nEpoch 28/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 91ms/step - accuracy: 0.4612 - loss: 1.3567 - val_accuracy: 0.4543 - val_loss: 1.3101\nEpoch 29/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 92ms/step - accuracy: 0.4637 - loss: 1.3648 - val_accuracy: 0.5007 - val_loss: 1.2382\nEpoch 30/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 92ms/step - accuracy: 0.4719 - loss: 1.3034 - val_accuracy: 0.4846 - val_loss: 1.2435\nEpoch 31/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 92ms/step - accuracy: 0.4863 - loss: 1.2784 - val_accuracy: 0.4145 - val_loss: 1.4510\nEpoch 32/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 91ms/step - accuracy: 0.4981 - loss: 1.2616 - val_accuracy: 0.4971 - val_loss: 24.9067\nEpoch 33/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 91ms/step - accuracy: 0.5111 - loss: 1.2307 - val_accuracy: 0.4664 - val_loss: 1.3227\nEpoch 34/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 95ms/step - accuracy: 0.5141 - loss: 1.2206 - val_accuracy: 0.5311 - val_loss: 1.1654\nEpoch 35/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 93ms/step - accuracy: 0.5243 - loss: 1.1991 - val_accuracy: 0.5427 - val_loss: 1.1634\nEpoch 36/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 93ms/step - accuracy: 0.5291 - loss: 1.1900 - val_accuracy: 0.5287 - val_loss: 1.1684\nEpoch 37/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 96ms/step - accuracy: 0.5372 - loss: 1.1820 - val_accuracy: 0.5136 - val_loss: 1.2210\nEpoch 38/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 90ms/step - accuracy: 0.5414 - loss: 1.1576 - val_accuracy: 0.5702 - val_loss: 1.1287\nEpoch 39/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 90ms/step - accuracy: 0.5606 - loss: 1.1469 - val_accuracy: 0.5584 - val_loss: 1.1405\nEpoch 40/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 92ms/step - accuracy: 0.5636 - loss: 1.1361 - val_accuracy: 0.5687 - val_loss: 1.1347\nEpoch 41/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 92ms/step - accuracy: 0.5701 - loss: 1.1216 - val_accuracy: 0.5798 - val_loss: 1.1086\nEpoch 42/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 90ms/step - accuracy: 0.5819 - loss: 1.1044 - val_accuracy: 0.5774 - val_loss: 1.0986\nEpoch 43/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 91ms/step - accuracy: 0.5854 - loss: 1.1048 - val_accuracy: 0.5899 - val_loss: 1.0722\nEpoch 44/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 91ms/step - accuracy: 0.5910 - loss: 1.0856 - val_accuracy: 0.5892 - val_loss: 1.1004\nEpoch 45/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 91ms/step - accuracy: 0.6000 - loss: 1.0727 - val_accuracy: 0.5862 - val_loss: 1.0950\nEpoch 46/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 96ms/step - accuracy: 0.6073 - loss: 1.0714 - val_accuracy: 0.6016 - val_loss: 1.0933\nEpoch 47/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 92ms/step - accuracy: 0.6204 - loss: 1.0379 - val_accuracy: 0.6100 - val_loss: 1.0657\nEpoch 48/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 95ms/step - accuracy: 0.6189 - loss: 1.0317 - val_accuracy: 0.6031 - val_loss: 1.0747\nEpoch 49/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 87ms/step - accuracy: 0.6254 - loss: 1.0226 - val_accuracy: 0.6090 - val_loss: 1.0558\nEpoch 50/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 90ms/step - accuracy: 0.6374 - loss: 0.9985 - val_accuracy: 0.6143 - val_loss: 1.0432\nEpoch 51/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 92ms/step - accuracy: 0.6376 - loss: 0.9781 - val_accuracy: 0.6059 - val_loss: 1.0676\nEpoch 52/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 91ms/step - accuracy: 0.6453 - loss: 0.9773 - val_accuracy: 0.6204 - val_loss: 1.0202\nEpoch 53/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 94ms/step - accuracy: 0.6589 - loss: 0.9473 - val_accuracy: 0.6310 - val_loss: 1.0054\nEpoch 54/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 93ms/step - accuracy: 0.6540 - loss: 0.9479 - val_accuracy: 0.6337 - val_loss: 1.0074\nEpoch 55/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 92ms/step - accuracy: 0.6591 - loss: 0.9372 - val_accuracy: 0.6199 - val_loss: 1.0375\nEpoch 56/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 90ms/step - accuracy: 0.6727 - loss: 0.9179 - val_accuracy: 0.6326 - val_loss: 1.0108\nEpoch 57/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 90ms/step - accuracy: 0.6745 - loss: 0.9077 - val_accuracy: 0.6395 - val_loss: 0.9907\nEpoch 58/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 91ms/step - accuracy: 0.6780 - loss: 0.8928 - val_accuracy: 0.6413 - val_loss: 1.0059\nEpoch 59/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 98ms/step - accuracy: 0.6782 - loss: 0.8898 - val_accuracy: 0.6320 - val_loss: 1.0085\nEpoch 60/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 97ms/step - accuracy: 0.6803 - loss: 0.8895 - val_accuracy: 0.6444 - val_loss: 0.9827\nEpoch 61/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 92ms/step - accuracy: 0.6897 - loss: 0.8698 - val_accuracy: 0.6488 - val_loss: 0.9728\nEpoch 62/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 89ms/step - accuracy: 0.6928 - loss: 0.8511 - val_accuracy: 0.6342 - val_loss: 1.0385\nEpoch 63/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 90ms/step - accuracy: 0.6911 - loss: 0.8667 - val_accuracy: 0.6538 - val_loss: 0.9633\nEpoch 64/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 89ms/step - accuracy: 0.7075 - loss: 0.8213 - val_accuracy: 0.6541 - val_loss: 0.9648\nEpoch 65/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 91ms/step - accuracy: 0.7071 - loss: 0.8242 - val_accuracy: 0.6413 - val_loss: 1.0116\nEpoch 66/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 88ms/step - accuracy: 0.7117 - loss: 0.8049 - val_accuracy: 0.6492 - val_loss: 0.9847\nEpoch 67/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 87ms/step - accuracy: 0.7081 - loss: 0.8236 - val_accuracy: 0.6512 - val_loss: 0.9948\nEpoch 68/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 89ms/step - accuracy: 0.7200 - loss: 0.7963 - val_accuracy: 0.6544 - val_loss: 0.9656\nEpoch 69/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 87ms/step - accuracy: 0.7187 - loss: 0.7796 - val_accuracy: 0.6527 - val_loss: 1.0001\nEpoch 70/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 88ms/step - accuracy: 0.7192 - loss: 0.7914 - val_accuracy: 0.6594 - val_loss: 0.9506\nEpoch 71/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 91ms/step - accuracy: 0.7258 - loss: 0.7768 - val_accuracy: 0.6544 - val_loss: 502.4883\nEpoch 72/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 94ms/step - accuracy: 0.7318 - loss: 0.7634 - val_accuracy: 0.6616 - val_loss: 0.9528\nEpoch 73/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 90ms/step - accuracy: 0.7304 - loss: 0.7605 - val_accuracy: 0.6465 - val_loss: 1.0246\nEpoch 74/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 90ms/step - accuracy: 0.7383 - loss: 0.7436 - val_accuracy: 0.6642 - val_loss: 0.9799\nEpoch 75/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 92ms/step - accuracy: 0.7378 - loss: 0.7466 - val_accuracy: 0.6647 - val_loss: 0.9517\nEpoch 76/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 92ms/step - accuracy: 0.7420 - loss: 0.7408 - val_accuracy: 0.6719 - val_loss: 0.9498\nEpoch 77/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 91ms/step - accuracy: 0.7443 - loss: 0.7298 - val_accuracy: 0.6710 - val_loss: 0.9657\nEpoch 78/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 99ms/step - accuracy: 0.7457 - loss: 0.7160 - val_accuracy: 0.6752 - val_loss: 0.9445\nEpoch 79/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 96ms/step - accuracy: 0.7529 - loss: 0.7038 - val_accuracy: 0.6729 - val_loss: 0.9547\nEpoch 80/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 89ms/step - accuracy: 0.7498 - loss: 0.7170 - val_accuracy: 0.6687 - val_loss: 0.9685\nEpoch 81/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 87ms/step - accuracy: 0.7503 - loss: 0.7053 - val_accuracy: 0.6634 - val_loss: 0.9964\nEpoch 82/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 94ms/step - accuracy: 0.7550 - loss: 0.7002 - val_accuracy: 0.6672 - val_loss: 0.9782\nEpoch 83/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 97ms/step - accuracy: 0.7583 - loss: 0.6843 - val_accuracy: 0.5972 - val_loss: 1.1832\nEpoch 84/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 96ms/step - accuracy: 0.7515 - loss: 0.7025 - val_accuracy: 0.5912 - val_loss: 1.1657\nEpoch 85/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 95ms/step - accuracy: 0.7576 - loss: 0.6894 - val_accuracy: 0.6624 - val_loss: 0.9902\nEpoch 86/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 92ms/step - accuracy: 0.7772 - loss: 0.6443 - val_accuracy: 0.6717 - val_loss: 0.9916\nEpoch 87/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 93ms/step - accuracy: 0.7758 - loss: 0.6415 - val_accuracy: 0.6554 - val_loss: 1.0348\nEpoch 88/720\n\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 94ms/step - accuracy: 0.7816 - loss: 0.6324 - val_accuracy: 0.6800 - val_loss: 0.9883\nEpoch 88: early stopping\n\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.6731 - loss: 1.0001\n","output_type":"stream"}]}]}